{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hfYbHgghOW5g"
      },
      "source": [
        "# Importação das bibliotecas básicas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IyRCfZ_fhLef"
      },
      "outputs": [],
      "source": [
        "!pip -q install plotly"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WOUlbp8VUhTT"
      },
      "outputs": [],
      "source": [
        "!pip -q install yellowbrick"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nVP1Ah9dhd1F"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import plotly.express as px\n",
        "import pickle"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neural_network import MLPClassifier\n",
        "import pickle\n",
        "from sklearn.metrics import accuracy_score\n",
        "from yellowbrick.classifier import ConfusionMatrix\n",
        "from sklearn.metrics import classification_report"
      ],
      "metadata": {
        "id": "4mPRzjBW3bKr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fSNUSSeOhqAd"
      },
      "source": [
        "# Redes neurais artificiais"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FvdcLLQLnFIW"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C6uL7hyzm8Dd"
      },
      "outputs": [],
      "source": [
        "# Abrir o arquivo pickle e carregar os dados de treinamento e teste\n",
        "with open('/content/drive/MyDrive/ML e Data Sciece com python/dataset/credit_risc/credit.pkl', 'rb') as f:\n",
        "  X_credit_treinamento, y_credit_treinamento, X_credit_teste, y_credit_teste = pickle.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SIVKNjPom-Dr",
        "outputId": "199ebc71-d4ec-477c-9692-22e86342ddc7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Estrutura do conjunto de treinamento de features: (1500, 3)\n",
            "Estrutura do conjunto de treinamento de rótulos: (1500,)\n"
          ]
        }
      ],
      "source": [
        "# Verificar a forma dos conjuntos de dados de treinamento\n",
        "print(\"Estrutura do conjunto de treinamento de features:\", X_credit_treinamento.shape)\n",
        "print(\"Estrutura do conjunto de treinamento de rótulos:\", y_credit_treinamento.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Df5EfDyjnAB6",
        "outputId": "f3038cd5-b3a1-443e-81f8-7d46afeb3104"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Estrutura do conjunto de teste de features: (500, 3)\n",
            "Estrutura do conjunto de teste de rótulos: (500,)\n"
          ]
        }
      ],
      "source": [
        "# Verificar a estrutura dos conjuntos de dados de teste\n",
        "print(\"Estrutura do conjunto de teste de features:\", X_credit_teste.shape)\n",
        "print(\"Estrutura do conjunto de teste de rótulos:\", y_credit_teste.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0c7fjRs-qePa",
        "outputId": "10baecb5-efdd-4376-b950-0df3b80af8ad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Quantidade aproximada de neuronio da camada oculta:  2.0\n"
          ]
        }
      ],
      "source": [
        "# regra empírica simples para estimar o número de neurônios em uma camada oculta de uma rede neural NÃO É UMA VERDADE ABSOLUTA APENAS UMA IDEIA DE APROXIMAÇÃO\n",
        "neuronio = (3 + 1) / 2\n",
        "print(\"Quantidade aproximada de neuronio da camada oculta: \", neuronio)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* A entrada é o número de características (ou dimensões) nos seus dados de entrada.\n",
        "* A saída é o número de classes ou o número de valores que você está tentando prever."
      ],
      "metadata": {
        "id": "RdX32UX0zWNE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "BCGNa_d8nYOf",
        "outputId": "e6f4fd48-9910-45de-a472-3e515c3dc81c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 0.87191396\n",
            "Iteration 2, loss = 0.77826008\n",
            "Iteration 3, loss = 0.70377598\n",
            "Iteration 4, loss = 0.64364862\n",
            "Iteration 5, loss = 0.59454229\n",
            "Iteration 6, loss = 0.55247774\n",
            "Iteration 7, loss = 0.51646962\n",
            "Iteration 8, loss = 0.48401268\n",
            "Iteration 9, loss = 0.45393224\n",
            "Iteration 10, loss = 0.42662974\n",
            "Iteration 11, loss = 0.40122726\n",
            "Iteration 12, loss = 0.37722580\n",
            "Iteration 13, loss = 0.35576569\n",
            "Iteration 14, loss = 0.33482244\n",
            "Iteration 15, loss = 0.31542024\n",
            "Iteration 16, loss = 0.29739832\n",
            "Iteration 17, loss = 0.28000805\n",
            "Iteration 18, loss = 0.26368319\n",
            "Iteration 19, loss = 0.24811205\n",
            "Iteration 20, loss = 0.23353651\n",
            "Iteration 21, loss = 0.21976635\n",
            "Iteration 22, loss = 0.20710924\n",
            "Iteration 23, loss = 0.19542908\n",
            "Iteration 24, loss = 0.18461138\n",
            "Iteration 25, loss = 0.17471887\n",
            "Iteration 26, loss = 0.16561762\n",
            "Iteration 27, loss = 0.15715572\n",
            "Iteration 28, loss = 0.14957851\n",
            "Iteration 29, loss = 0.14263838\n",
            "Iteration 30, loss = 0.13662332\n",
            "Iteration 31, loss = 0.13089245\n",
            "Iteration 32, loss = 0.12564109\n",
            "Iteration 33, loss = 0.12068861\n",
            "Iteration 34, loss = 0.11617302\n",
            "Iteration 35, loss = 0.11192565\n",
            "Iteration 36, loss = 0.10784740\n",
            "Iteration 37, loss = 0.10421227\n",
            "Iteration 38, loss = 0.10088519\n",
            "Iteration 39, loss = 0.09762784\n",
            "Iteration 40, loss = 0.09453177\n",
            "Iteration 41, loss = 0.09168947\n",
            "Iteration 42, loss = 0.08895451\n",
            "Iteration 43, loss = 0.08632076\n",
            "Iteration 44, loss = 0.08401498\n",
            "Iteration 45, loss = 0.08183261\n",
            "Iteration 46, loss = 0.07967526\n",
            "Iteration 47, loss = 0.07768815\n",
            "Iteration 48, loss = 0.07584766\n",
            "Iteration 49, loss = 0.07414780\n",
            "Iteration 50, loss = 0.07260678\n",
            "Iteration 51, loss = 0.07112424\n",
            "Iteration 52, loss = 0.06962304\n",
            "Iteration 53, loss = 0.06835194\n",
            "Iteration 54, loss = 0.06697736\n",
            "Iteration 55, loss = 0.06576915\n",
            "Iteration 56, loss = 0.06473693\n",
            "Iteration 57, loss = 0.06346612\n",
            "Iteration 58, loss = 0.06243177\n",
            "Iteration 59, loss = 0.06120420\n",
            "Iteration 60, loss = 0.06020310\n",
            "Iteration 61, loss = 0.05927795\n",
            "Iteration 62, loss = 0.05830262\n",
            "Iteration 63, loss = 0.05741282\n",
            "Iteration 64, loss = 0.05649288\n",
            "Iteration 65, loss = 0.05558440\n",
            "Iteration 66, loss = 0.05483962\n",
            "Iteration 67, loss = 0.05406062\n",
            "Iteration 68, loss = 0.05325396\n",
            "Iteration 69, loss = 0.05249971\n",
            "Iteration 70, loss = 0.05172613\n",
            "Iteration 71, loss = 0.05105726\n",
            "Iteration 72, loss = 0.05035259\n",
            "Iteration 73, loss = 0.04960247\n",
            "Iteration 74, loss = 0.04896240\n",
            "Iteration 75, loss = 0.04829300\n",
            "Iteration 76, loss = 0.04768846\n",
            "Iteration 77, loss = 0.04706842\n",
            "Iteration 78, loss = 0.04649389\n",
            "Iteration 79, loss = 0.04584719\n",
            "Iteration 80, loss = 0.04526429\n",
            "Iteration 81, loss = 0.04478856\n",
            "Iteration 82, loss = 0.04428023\n",
            "Iteration 83, loss = 0.04362460\n",
            "Iteration 84, loss = 0.04314738\n",
            "Iteration 85, loss = 0.04259423\n",
            "Iteration 86, loss = 0.04204113\n",
            "Iteration 87, loss = 0.04146416\n",
            "Iteration 88, loss = 0.04105859\n",
            "Iteration 89, loss = 0.04052726\n",
            "Iteration 90, loss = 0.03999200\n",
            "Iteration 91, loss = 0.03959523\n",
            "Iteration 92, loss = 0.03905825\n",
            "Iteration 93, loss = 0.03865788\n",
            "Iteration 94, loss = 0.03812923\n",
            "Iteration 95, loss = 0.03776898\n",
            "Iteration 96, loss = 0.03722940\n",
            "Iteration 97, loss = 0.03670991\n",
            "Iteration 98, loss = 0.03635228\n",
            "Iteration 99, loss = 0.03578082\n",
            "Iteration 100, loss = 0.03537667\n",
            "Iteration 101, loss = 0.03493114\n",
            "Iteration 102, loss = 0.03448707\n",
            "Iteration 103, loss = 0.03410355\n",
            "Iteration 104, loss = 0.03383815\n",
            "Iteration 105, loss = 0.03325977\n",
            "Iteration 106, loss = 0.03294212\n",
            "Iteration 107, loss = 0.03264368\n",
            "Iteration 108, loss = 0.03222523\n",
            "Iteration 109, loss = 0.03188717\n",
            "Iteration 110, loss = 0.03164964\n",
            "Iteration 111, loss = 0.03149636\n",
            "Iteration 112, loss = 0.03098012\n",
            "Iteration 113, loss = 0.03051787\n",
            "Iteration 114, loss = 0.03027365\n",
            "Iteration 115, loss = 0.03004261\n",
            "Iteration 116, loss = 0.02978278\n",
            "Iteration 117, loss = 0.02943460\n",
            "Iteration 118, loss = 0.02915147\n",
            "Iteration 119, loss = 0.02889705\n",
            "Iteration 120, loss = 0.02863082\n",
            "Iteration 121, loss = 0.02845858\n",
            "Iteration 122, loss = 0.02809312\n",
            "Iteration 123, loss = 0.02789818\n",
            "Iteration 124, loss = 0.02767819\n",
            "Iteration 125, loss = 0.02743042\n",
            "Iteration 126, loss = 0.02713983\n",
            "Iteration 127, loss = 0.02697157\n",
            "Iteration 128, loss = 0.02672384\n",
            "Iteration 129, loss = 0.02650707\n",
            "Iteration 130, loss = 0.02628401\n",
            "Iteration 131, loss = 0.02606321\n",
            "Iteration 132, loss = 0.02582697\n",
            "Iteration 133, loss = 0.02557730\n",
            "Iteration 134, loss = 0.02544747\n",
            "Iteration 135, loss = 0.02523074\n",
            "Iteration 136, loss = 0.02499608\n",
            "Iteration 137, loss = 0.02487907\n",
            "Iteration 138, loss = 0.02468212\n",
            "Iteration 139, loss = 0.02448774\n",
            "Iteration 140, loss = 0.02423354\n",
            "Iteration 141, loss = 0.02408462\n",
            "Iteration 142, loss = 0.02391390\n",
            "Iteration 143, loss = 0.02367746\n",
            "Iteration 144, loss = 0.02345992\n",
            "Iteration 145, loss = 0.02332977\n",
            "Iteration 146, loss = 0.02326477\n",
            "Iteration 147, loss = 0.02292917\n",
            "Iteration 148, loss = 0.02284890\n",
            "Iteration 149, loss = 0.02265899\n",
            "Iteration 150, loss = 0.02250923\n",
            "Iteration 151, loss = 0.02240317\n",
            "Iteration 152, loss = 0.02219135\n",
            "Iteration 153, loss = 0.02210319\n",
            "Iteration 154, loss = 0.02185844\n",
            "Iteration 155, loss = 0.02164341\n",
            "Iteration 156, loss = 0.02154623\n",
            "Iteration 157, loss = 0.02149028\n",
            "Iteration 158, loss = 0.02121680\n",
            "Iteration 159, loss = 0.02125705\n",
            "Iteration 160, loss = 0.02097341\n",
            "Iteration 161, loss = 0.02079674\n",
            "Iteration 162, loss = 0.02072893\n",
            "Iteration 163, loss = 0.02050259\n",
            "Iteration 164, loss = 0.02048829\n",
            "Iteration 165, loss = 0.02022237\n",
            "Iteration 166, loss = 0.02027435\n",
            "Iteration 167, loss = 0.02005169\n",
            "Iteration 168, loss = 0.01997607\n",
            "Iteration 169, loss = 0.01980372\n",
            "Iteration 170, loss = 0.01963064\n",
            "Iteration 171, loss = 0.01945530\n",
            "Iteration 172, loss = 0.01944674\n",
            "Iteration 173, loss = 0.01926922\n",
            "Iteration 174, loss = 0.01922199\n",
            "Iteration 175, loss = 0.01899704\n",
            "Iteration 176, loss = 0.01898926\n",
            "Iteration 177, loss = 0.01890100\n",
            "Iteration 178, loss = 0.01864992\n",
            "Iteration 179, loss = 0.01852996\n",
            "Iteration 180, loss = 0.01842868\n",
            "Iteration 181, loss = 0.01833652\n",
            "Iteration 182, loss = 0.01821010\n",
            "Iteration 183, loss = 0.01821220\n",
            "Iteration 184, loss = 0.01800125\n",
            "Iteration 185, loss = 0.01783948\n",
            "Iteration 186, loss = 0.01773943\n",
            "Iteration 187, loss = 0.01776865\n",
            "Iteration 188, loss = 0.01761989\n",
            "Iteration 189, loss = 0.01755282\n",
            "Iteration 190, loss = 0.01733565\n",
            "Iteration 191, loss = 0.01727760\n",
            "Iteration 192, loss = 0.01713235\n",
            "Iteration 193, loss = 0.01699720\n",
            "Iteration 194, loss = 0.01711479\n",
            "Iteration 195, loss = 0.01687960\n",
            "Iteration 196, loss = 0.01680448\n",
            "Iteration 197, loss = 0.01661644\n",
            "Iteration 198, loss = 0.01658896\n",
            "Iteration 199, loss = 0.01642860\n",
            "Iteration 200, loss = 0.01636221\n",
            "Iteration 201, loss = 0.01627757\n",
            "Iteration 202, loss = 0.01618204\n",
            "Iteration 203, loss = 0.01607962\n",
            "Iteration 204, loss = 0.01595477\n",
            "Iteration 205, loss = 0.01587002\n",
            "Iteration 206, loss = 0.01583678\n",
            "Iteration 207, loss = 0.01568716\n",
            "Iteration 208, loss = 0.01569760\n",
            "Iteration 209, loss = 0.01555824\n",
            "Iteration 210, loss = 0.01551451\n",
            "Iteration 211, loss = 0.01543881\n",
            "Iteration 212, loss = 0.01539720\n",
            "Iteration 213, loss = 0.01528630\n",
            "Iteration 214, loss = 0.01514668\n",
            "Iteration 215, loss = 0.01506660\n",
            "Iteration 216, loss = 0.01502056\n",
            "Iteration 217, loss = 0.01495697\n",
            "Iteration 218, loss = 0.01476482\n",
            "Iteration 219, loss = 0.01476433\n",
            "Iteration 220, loss = 0.01465570\n",
            "Iteration 221, loss = 0.01464377\n",
            "Iteration 222, loss = 0.01447432\n",
            "Iteration 223, loss = 0.01457780\n",
            "Iteration 224, loss = 0.01445229\n",
            "Iteration 225, loss = 0.01425959\n",
            "Iteration 226, loss = 0.01419299\n",
            "Iteration 227, loss = 0.01421137\n",
            "Iteration 228, loss = 0.01401482\n",
            "Iteration 229, loss = 0.01398566\n",
            "Iteration 230, loss = 0.01390347\n",
            "Iteration 231, loss = 0.01380725\n",
            "Iteration 232, loss = 0.01377031\n",
            "Iteration 233, loss = 0.01368710\n",
            "Iteration 234, loss = 0.01361255\n",
            "Iteration 235, loss = 0.01353277\n",
            "Iteration 236, loss = 0.01346639\n",
            "Iteration 237, loss = 0.01337099\n",
            "Iteration 238, loss = 0.01333289\n",
            "Iteration 239, loss = 0.01338287\n",
            "Iteration 240, loss = 0.01323168\n",
            "Iteration 241, loss = 0.01324306\n",
            "Iteration 242, loss = 0.01303883\n",
            "Iteration 243, loss = 0.01303318\n",
            "Iteration 244, loss = 0.01307515\n",
            "Iteration 245, loss = 0.01281095\n",
            "Iteration 246, loss = 0.01298384\n",
            "Iteration 247, loss = 0.01274255\n",
            "Iteration 248, loss = 0.01278551\n",
            "Iteration 249, loss = 0.01267938\n",
            "Iteration 250, loss = 0.01266363\n",
            "Iteration 251, loss = 0.01255776\n",
            "Iteration 252, loss = 0.01248271\n",
            "Iteration 253, loss = 0.01241151\n",
            "Iteration 254, loss = 0.01241168\n",
            "Iteration 255, loss = 0.01248079\n",
            "Iteration 256, loss = 0.01232025\n",
            "Iteration 257, loss = 0.01229430\n",
            "Iteration 258, loss = 0.01216196\n",
            "Iteration 259, loss = 0.01217527\n",
            "Iteration 260, loss = 0.01200186\n",
            "Iteration 261, loss = 0.01193462\n",
            "Iteration 262, loss = 0.01192703\n",
            "Iteration 263, loss = 0.01182184\n",
            "Iteration 264, loss = 0.01187336\n",
            "Iteration 265, loss = 0.01177501\n",
            "Iteration 266, loss = 0.01163757\n",
            "Iteration 267, loss = 0.01163067\n",
            "Iteration 268, loss = 0.01155712\n",
            "Iteration 269, loss = 0.01151240\n",
            "Iteration 270, loss = 0.01144647\n",
            "Iteration 271, loss = 0.01143119\n",
            "Iteration 272, loss = 0.01139256\n",
            "Iteration 273, loss = 0.01131782\n",
            "Iteration 274, loss = 0.01126698\n",
            "Iteration 275, loss = 0.01130471\n",
            "Iteration 276, loss = 0.01114173\n",
            "Iteration 277, loss = 0.01113562\n",
            "Iteration 278, loss = 0.01114502\n",
            "Iteration 279, loss = 0.01099204\n",
            "Iteration 280, loss = 0.01103142\n",
            "Iteration 281, loss = 0.01100442\n",
            "Iteration 282, loss = 0.01081111\n",
            "Iteration 283, loss = 0.01084993\n",
            "Iteration 284, loss = 0.01070875\n",
            "Iteration 285, loss = 0.01064358\n",
            "Iteration 286, loss = 0.01060586\n",
            "Iteration 287, loss = 0.01092684\n",
            "Iteration 288, loss = 0.01056579\n",
            "Iteration 289, loss = 0.01053518\n",
            "Iteration 290, loss = 0.01040560\n",
            "Iteration 291, loss = 0.01036482\n",
            "Iteration 292, loss = 0.01040479\n",
            "Iteration 293, loss = 0.01042728\n",
            "Iteration 294, loss = 0.01024155\n",
            "Iteration 295, loss = 0.01015249\n",
            "Iteration 296, loss = 0.01018221\n",
            "Iteration 297, loss = 0.01011575\n",
            "Iteration 298, loss = 0.01019114\n",
            "Iteration 299, loss = 0.01001713\n",
            "Iteration 300, loss = 0.01000647\n",
            "Iteration 301, loss = 0.00991158\n",
            "Iteration 302, loss = 0.01001926\n",
            "Iteration 303, loss = 0.00991583\n",
            "Iteration 304, loss = 0.00986499\n",
            "Iteration 305, loss = 0.00984954\n",
            "Iteration 306, loss = 0.00975458\n",
            "Iteration 307, loss = 0.00972997\n",
            "Iteration 308, loss = 0.00973841\n",
            "Iteration 309, loss = 0.00965414\n",
            "Iteration 310, loss = 0.00956948\n",
            "Iteration 311, loss = 0.00951773\n",
            "Iteration 312, loss = 0.00950046\n",
            "Iteration 313, loss = 0.00943587\n",
            "Iteration 314, loss = 0.00938001\n",
            "Iteration 315, loss = 0.00941908\n",
            "Iteration 316, loss = 0.00955165\n",
            "Iteration 317, loss = 0.00921606\n",
            "Iteration 318, loss = 0.00925920\n",
            "Iteration 319, loss = 0.00921003\n",
            "Iteration 320, loss = 0.00913701\n",
            "Iteration 321, loss = 0.00931362\n",
            "Iteration 322, loss = 0.00912935\n",
            "Iteration 323, loss = 0.00907189\n",
            "Iteration 324, loss = 0.00918228\n",
            "Iteration 325, loss = 0.00901782\n",
            "Iteration 326, loss = 0.00907208\n",
            "Iteration 327, loss = 0.00905237\n",
            "Iteration 328, loss = 0.00890103\n",
            "Iteration 329, loss = 0.00878831\n",
            "Iteration 330, loss = 0.00879981\n",
            "Iteration 331, loss = 0.00879152\n",
            "Iteration 332, loss = 0.00876238\n",
            "Iteration 333, loss = 0.00862228\n",
            "Iteration 334, loss = 0.00877881\n",
            "Iteration 335, loss = 0.00859048\n",
            "Iteration 336, loss = 0.00868152\n",
            "Iteration 337, loss = 0.00851051\n",
            "Iteration 338, loss = 0.00858770\n",
            "Iteration 339, loss = 0.00843545\n",
            "Iteration 340, loss = 0.00837402\n",
            "Iteration 341, loss = 0.00849993\n",
            "Iteration 342, loss = 0.00841078\n",
            "Iteration 343, loss = 0.00831818\n",
            "Iteration 344, loss = 0.00828436\n",
            "Iteration 345, loss = 0.00839116\n",
            "Iteration 346, loss = 0.00828295\n",
            "Iteration 347, loss = 0.00824119\n",
            "Iteration 348, loss = 0.00838090\n",
            "Iteration 349, loss = 0.00817668\n",
            "Iteration 350, loss = 0.00813587\n",
            "Iteration 351, loss = 0.00809250\n",
            "Iteration 352, loss = 0.00804321\n",
            "Iteration 353, loss = 0.00810815\n",
            "Iteration 354, loss = 0.00796527\n",
            "Iteration 355, loss = 0.00795634\n",
            "Iteration 356, loss = 0.00792938\n",
            "Iteration 357, loss = 0.00787625\n",
            "Iteration 358, loss = 0.00786040\n",
            "Iteration 359, loss = 0.00793032\n",
            "Iteration 360, loss = 0.00779256\n",
            "Iteration 361, loss = 0.00776137\n",
            "Iteration 362, loss = 0.00785695\n",
            "Iteration 363, loss = 0.00768154\n",
            "Iteration 364, loss = 0.00768667\n",
            "Iteration 365, loss = 0.00764841\n",
            "Iteration 366, loss = 0.00778927\n",
            "Iteration 367, loss = 0.00763680\n",
            "Iteration 368, loss = 0.00756399\n",
            "Iteration 369, loss = 0.00749638\n",
            "Iteration 370, loss = 0.00745953\n",
            "Iteration 371, loss = 0.00749919\n",
            "Iteration 372, loss = 0.00744702\n",
            "Iteration 373, loss = 0.00736922\n",
            "Iteration 374, loss = 0.00741668\n",
            "Iteration 375, loss = 0.00734850\n",
            "Iteration 376, loss = 0.00733189\n",
            "Iteration 377, loss = 0.00736412\n",
            "Iteration 378, loss = 0.00720969\n",
            "Iteration 379, loss = 0.00722912\n",
            "Iteration 380, loss = 0.00717040\n",
            "Iteration 381, loss = 0.00718884\n",
            "Iteration 382, loss = 0.00710335\n",
            "Iteration 383, loss = 0.00711872\n",
            "Iteration 384, loss = 0.00703060\n",
            "Iteration 385, loss = 0.00702791\n",
            "Iteration 386, loss = 0.00718388\n",
            "Iteration 387, loss = 0.00697712\n",
            "Iteration 388, loss = 0.00696708\n",
            "Iteration 389, loss = 0.00691965\n",
            "Iteration 390, loss = 0.00690326\n",
            "Iteration 391, loss = 0.00697113\n",
            "Iteration 392, loss = 0.00687642\n",
            "Iteration 393, loss = 0.00681968\n",
            "Iteration 394, loss = 0.00684496\n",
            "Iteration 395, loss = 0.00692181\n",
            "Iteration 396, loss = 0.00670787\n",
            "Iteration 397, loss = 0.00677855\n",
            "Iteration 398, loss = 0.00672594\n",
            "Iteration 399, loss = 0.00672026\n",
            "Iteration 400, loss = 0.00666819\n",
            "Iteration 401, loss = 0.00672096\n",
            "Iteration 402, loss = 0.00677167\n",
            "Iteration 403, loss = 0.00656954\n",
            "Iteration 404, loss = 0.00660657\n",
            "Iteration 405, loss = 0.00653181\n",
            "Iteration 406, loss = 0.00662476\n",
            "Iteration 407, loss = 0.00654866\n",
            "Iteration 408, loss = 0.00656172\n",
            "Iteration 409, loss = 0.00645134\n",
            "Iteration 410, loss = 0.00635443\n",
            "Iteration 411, loss = 0.00641101\n",
            "Iteration 412, loss = 0.00640055\n",
            "Iteration 413, loss = 0.00638284\n",
            "Iteration 414, loss = 0.00628525\n",
            "Iteration 415, loss = 0.00639408\n",
            "Iteration 416, loss = 0.00617743\n",
            "Iteration 417, loss = 0.00632729\n",
            "Iteration 418, loss = 0.00619579\n",
            "Iteration 419, loss = 0.00621089\n",
            "Iteration 420, loss = 0.00621934\n",
            "Iteration 421, loss = 0.00613865\n",
            "Iteration 422, loss = 0.00618779\n",
            "Iteration 423, loss = 0.00617926\n",
            "Iteration 424, loss = 0.00607293\n",
            "Iteration 425, loss = 0.00629558\n",
            "Iteration 426, loss = 0.00613389\n",
            "Iteration 427, loss = 0.00599668\n",
            "Iteration 428, loss = 0.00605678\n",
            "Iteration 429, loss = 0.00595083\n",
            "Iteration 430, loss = 0.00597681\n",
            "Iteration 431, loss = 0.00587831\n",
            "Iteration 432, loss = 0.00586115\n",
            "Iteration 433, loss = 0.00587398\n",
            "Iteration 434, loss = 0.00581061\n",
            "Iteration 435, loss = 0.00583211\n",
            "Iteration 436, loss = 0.00582317\n",
            "Iteration 437, loss = 0.00583079\n",
            "Iteration 438, loss = 0.00575445\n",
            "Iteration 439, loss = 0.00572591\n",
            "Iteration 440, loss = 0.00577102\n",
            "Iteration 441, loss = 0.00572104\n",
            "Iteration 442, loss = 0.00569902\n",
            "Iteration 443, loss = 0.00575885\n",
            "Iteration 444, loss = 0.00563452\n",
            "Iteration 445, loss = 0.00565729\n",
            "Iteration 446, loss = 0.00555861\n",
            "Iteration 447, loss = 0.00556744\n",
            "Iteration 448, loss = 0.00551687\n",
            "Iteration 449, loss = 0.00562744\n",
            "Iteration 450, loss = 0.00559324\n",
            "Iteration 451, loss = 0.00550748\n",
            "Iteration 452, loss = 0.00542087\n",
            "Iteration 453, loss = 0.00541509\n",
            "Iteration 454, loss = 0.00541101\n",
            "Iteration 455, loss = 0.00543152\n",
            "Iteration 456, loss = 0.00538787\n",
            "Iteration 457, loss = 0.00546719\n",
            "Iteration 458, loss = 0.00533247\n",
            "Iteration 459, loss = 0.00543396\n",
            "Iteration 460, loss = 0.00527417\n",
            "Iteration 461, loss = 0.00531487\n",
            "Iteration 462, loss = 0.00527336\n",
            "Iteration 463, loss = 0.00532612\n",
            "Iteration 464, loss = 0.00523510\n",
            "Iteration 465, loss = 0.00518705\n",
            "Iteration 466, loss = 0.00524679\n",
            "Iteration 467, loss = 0.00524043\n",
            "Iteration 468, loss = 0.00527574\n",
            "Iteration 469, loss = 0.00526378\n",
            "Iteration 470, loss = 0.00512083\n",
            "Iteration 471, loss = 0.00532333\n",
            "Iteration 472, loss = 0.00511728\n",
            "Iteration 473, loss = 0.00509223\n",
            "Iteration 474, loss = 0.00507657\n",
            "Iteration 475, loss = 0.00502459\n",
            "Iteration 476, loss = 0.00504292\n",
            "Iteration 477, loss = 0.00494135\n",
            "Iteration 478, loss = 0.00502809\n",
            "Iteration 479, loss = 0.00502605\n",
            "Iteration 480, loss = 0.00495731\n",
            "Iteration 481, loss = 0.00487640\n",
            "Iteration 482, loss = 0.00492312\n",
            "Iteration 483, loss = 0.00478192\n",
            "Iteration 484, loss = 0.00510728\n",
            "Iteration 485, loss = 0.00484915\n",
            "Iteration 486, loss = 0.00485893\n",
            "Iteration 487, loss = 0.00487712\n",
            "Iteration 488, loss = 0.00480683\n",
            "Iteration 489, loss = 0.00482542\n",
            "Iteration 490, loss = 0.00478058\n",
            "Iteration 491, loss = 0.00483612\n",
            "Iteration 492, loss = 0.00472382\n",
            "Iteration 493, loss = 0.00474124\n",
            "Iteration 494, loss = 0.00464976\n",
            "Iteration 495, loss = 0.00468059\n",
            "Iteration 496, loss = 0.00463582\n",
            "Iteration 497, loss = 0.00463787\n",
            "Iteration 498, loss = 0.00460746\n",
            "Iteration 499, loss = 0.00462363\n",
            "Iteration 500, loss = 0.00466419\n",
            "Iteration 501, loss = 0.00459536\n",
            "Iteration 502, loss = 0.00454248\n",
            "Iteration 503, loss = 0.00454570\n",
            "Iteration 504, loss = 0.00453540\n",
            "Iteration 505, loss = 0.00448368\n",
            "Iteration 506, loss = 0.00455247\n",
            "Iteration 507, loss = 0.00446893\n",
            "Iteration 508, loss = 0.00458699\n",
            "Iteration 509, loss = 0.00444680\n",
            "Iteration 510, loss = 0.00445842\n",
            "Iteration 511, loss = 0.00443972\n",
            "Iteration 512, loss = 0.00452229\n",
            "Iteration 513, loss = 0.00441514\n",
            "Iteration 514, loss = 0.00436345\n",
            "Iteration 515, loss = 0.00443082\n",
            "Iteration 516, loss = 0.00439951\n",
            "Iteration 517, loss = 0.00449241\n",
            "Iteration 518, loss = 0.00440886\n",
            "Iteration 519, loss = 0.00426977\n",
            "Iteration 520, loss = 0.00430982\n",
            "Iteration 521, loss = 0.00427414\n",
            "Iteration 522, loss = 0.00425604\n",
            "Iteration 523, loss = 0.00426274\n",
            "Iteration 524, loss = 0.00421769\n",
            "Iteration 525, loss = 0.00426619\n",
            "Iteration 526, loss = 0.00420516\n",
            "Iteration 527, loss = 0.00423671\n",
            "Iteration 528, loss = 0.00419552\n",
            "Iteration 529, loss = 0.00421736\n",
            "Iteration 530, loss = 0.00415980\n",
            "Iteration 531, loss = 0.00413282\n",
            "Iteration 532, loss = 0.00408835\n",
            "Iteration 533, loss = 0.00410525\n",
            "Iteration 534, loss = 0.00409141\n",
            "Iteration 535, loss = 0.00404531\n",
            "Iteration 536, loss = 0.00410198\n",
            "Iteration 537, loss = 0.00408973\n",
            "Iteration 538, loss = 0.00405025\n",
            "Iteration 539, loss = 0.00400452\n",
            "Iteration 540, loss = 0.00403725\n",
            "Iteration 541, loss = 0.00397012\n",
            "Iteration 542, loss = 0.00400261\n",
            "Iteration 543, loss = 0.00396141\n",
            "Iteration 544, loss = 0.00407515\n",
            "Iteration 545, loss = 0.00393424\n",
            "Iteration 546, loss = 0.00394593\n",
            "Iteration 547, loss = 0.00390046\n",
            "Iteration 548, loss = 0.00394152\n",
            "Iteration 549, loss = 0.00384394\n",
            "Iteration 550, loss = 0.00389241\n",
            "Iteration 551, loss = 0.00386653\n",
            "Iteration 552, loss = 0.00388193\n",
            "Iteration 553, loss = 0.00385132\n",
            "Iteration 554, loss = 0.00376817\n",
            "Iteration 555, loss = 0.00379826\n",
            "Iteration 556, loss = 0.00378579\n",
            "Iteration 557, loss = 0.00382046\n",
            "Iteration 558, loss = 0.00376718\n",
            "Iteration 559, loss = 0.00381887\n",
            "Iteration 560, loss = 0.00374937\n",
            "Iteration 561, loss = 0.00370492\n",
            "Iteration 562, loss = 0.00398230\n",
            "Iteration 563, loss = 0.00381170\n",
            "Iteration 564, loss = 0.00391478\n",
            "Iteration 565, loss = 0.00374712\n",
            "Iteration 566, loss = 0.00366265\n",
            "Iteration 567, loss = 0.00367256\n",
            "Iteration 568, loss = 0.00375177\n",
            "Iteration 569, loss = 0.00353075\n",
            "Iteration 570, loss = 0.00364595\n",
            "Iteration 571, loss = 0.00374557\n",
            "Iteration 572, loss = 0.00354490\n",
            "Iteration 573, loss = 0.00365565\n",
            "Iteration 574, loss = 0.00366467\n",
            "Iteration 575, loss = 0.00365219\n",
            "Iteration 576, loss = 0.00357394\n",
            "Iteration 577, loss = 0.00348923\n",
            "Iteration 578, loss = 0.00364656\n",
            "Iteration 579, loss = 0.00350186\n",
            "Iteration 580, loss = 0.00359175\n",
            "Iteration 581, loss = 0.00349628\n",
            "Iteration 582, loss = 0.00353074\n",
            "Iteration 583, loss = 0.00344150\n",
            "Iteration 584, loss = 0.00350218\n",
            "Iteration 585, loss = 0.00343187\n",
            "Iteration 586, loss = 0.00340622\n",
            "Iteration 587, loss = 0.00343084\n",
            "Iteration 588, loss = 0.00341003\n",
            "Iteration 589, loss = 0.00347067\n",
            "Iteration 590, loss = 0.00337813\n",
            "Iteration 591, loss = 0.00342500\n",
            "Iteration 592, loss = 0.00342250\n",
            "Iteration 593, loss = 0.00335364\n",
            "Iteration 594, loss = 0.00332218\n",
            "Iteration 595, loss = 0.00336301\n",
            "Iteration 596, loss = 0.00326989\n",
            "Iteration 597, loss = 0.00349533\n",
            "Iteration 598, loss = 0.00345990\n",
            "Iteration 599, loss = 0.00332213\n",
            "Iteration 600, loss = 0.00329877\n",
            "Iteration 601, loss = 0.00327829\n",
            "Iteration 602, loss = 0.00332501\n",
            "Iteration 603, loss = 0.00346076\n",
            "Iteration 604, loss = 0.00325472\n",
            "Iteration 605, loss = 0.00323882\n",
            "Iteration 606, loss = 0.00321066\n",
            "Iteration 607, loss = 0.00319354\n",
            "Iteration 608, loss = 0.00320796\n",
            "Iteration 609, loss = 0.00315408\n",
            "Iteration 610, loss = 0.00317471\n",
            "Iteration 611, loss = 0.00315552\n",
            "Iteration 612, loss = 0.00318349\n",
            "Iteration 613, loss = 0.00327171\n",
            "Iteration 614, loss = 0.00307508\n",
            "Iteration 615, loss = 0.00311613\n",
            "Iteration 616, loss = 0.00312919\n",
            "Iteration 617, loss = 0.00307424\n",
            "Iteration 618, loss = 0.00310548\n",
            "Iteration 619, loss = 0.00304441\n",
            "Iteration 620, loss = 0.00305615\n",
            "Iteration 621, loss = 0.00304376\n",
            "Iteration 622, loss = 0.00304303\n",
            "Iteration 623, loss = 0.00305092\n",
            "Iteration 624, loss = 0.00307628\n",
            "Iteration 625, loss = 0.00301640\n",
            "Iteration 626, loss = 0.00306969\n",
            "Iteration 627, loss = 0.00319003\n",
            "Iteration 628, loss = 0.00310120\n",
            "Iteration 629, loss = 0.00300206\n",
            "Iteration 630, loss = 0.00296831\n",
            "Iteration 631, loss = 0.00297992\n",
            "Iteration 632, loss = 0.00290147\n",
            "Iteration 633, loss = 0.00299038\n",
            "Iteration 634, loss = 0.00297449\n",
            "Iteration 635, loss = 0.00299420\n",
            "Iteration 636, loss = 0.00290896\n",
            "Iteration 637, loss = 0.00293022\n",
            "Iteration 638, loss = 0.00295381\n",
            "Iteration 639, loss = 0.00293851\n",
            "Iteration 640, loss = 0.00301737\n",
            "Iteration 641, loss = 0.00288291\n",
            "Iteration 642, loss = 0.00293765\n",
            "Iteration 643, loss = 0.00290695\n",
            "Iteration 644, loss = 0.00282376\n",
            "Iteration 645, loss = 0.00282201\n",
            "Iteration 646, loss = 0.00280615\n",
            "Iteration 647, loss = 0.00279465\n",
            "Iteration 648, loss = 0.00291725\n",
            "Iteration 649, loss = 0.00303717\n",
            "Iteration 650, loss = 0.00290332\n",
            "Iteration 651, loss = 0.00284627\n",
            "Iteration 652, loss = 0.00274115\n",
            "Iteration 653, loss = 0.00276036\n",
            "Iteration 654, loss = 0.00277256\n",
            "Iteration 655, loss = 0.00283421\n",
            "Iteration 656, loss = 0.00273624\n",
            "Iteration 657, loss = 0.00270123\n",
            "Iteration 658, loss = 0.00270768\n",
            "Iteration 659, loss = 0.00268569\n",
            "Iteration 660, loss = 0.00281599\n",
            "Iteration 661, loss = 0.00267527\n",
            "Iteration 662, loss = 0.00265414\n",
            "Iteration 663, loss = 0.00273063\n",
            "Iteration 664, loss = 0.00281194\n",
            "Iteration 665, loss = 0.00289668\n",
            "Iteration 666, loss = 0.00274365\n",
            "Iteration 667, loss = 0.00269050\n",
            "Iteration 668, loss = 0.00257910\n",
            "Iteration 669, loss = 0.00265377\n",
            "Iteration 670, loss = 0.00262514\n",
            "Iteration 671, loss = 0.00261212\n",
            "Iteration 672, loss = 0.00266499\n",
            "Iteration 673, loss = 0.00273386\n",
            "Iteration 674, loss = 0.00261225\n",
            "Iteration 675, loss = 0.00254030\n",
            "Iteration 676, loss = 0.00258054\n",
            "Iteration 677, loss = 0.00261084\n",
            "Iteration 678, loss = 0.00257120\n",
            "Iteration 679, loss = 0.00251778\n",
            "Iteration 680, loss = 0.00251685\n",
            "Iteration 681, loss = 0.00256354\n",
            "Iteration 682, loss = 0.00251528\n",
            "Iteration 683, loss = 0.00258683\n",
            "Iteration 684, loss = 0.00255322\n",
            "Iteration 685, loss = 0.00252795\n",
            "Iteration 686, loss = 0.00244910\n",
            "Iteration 687, loss = 0.00253478\n",
            "Iteration 688, loss = 0.00247314\n",
            "Iteration 689, loss = 0.00246643\n",
            "Iteration 690, loss = 0.00246481\n",
            "Iteration 691, loss = 0.00244767\n",
            "Iteration 692, loss = 0.00237699\n",
            "Iteration 693, loss = 0.00258066\n",
            "Iteration 694, loss = 0.00248830\n",
            "Iteration 695, loss = 0.00247847\n",
            "Iteration 696, loss = 0.00236560\n",
            "Iteration 697, loss = 0.00245385\n",
            "Iteration 698, loss = 0.00248991\n",
            "Iteration 699, loss = 0.00238758\n",
            "Iteration 700, loss = 0.00235081\n",
            "Iteration 701, loss = 0.00238468\n",
            "Iteration 702, loss = 0.00234783\n",
            "Iteration 703, loss = 0.00237991\n",
            "Iteration 704, loss = 0.00237854\n",
            "Iteration 705, loss = 0.00233714\n",
            "Iteration 706, loss = 0.00236864\n",
            "Iteration 707, loss = 0.00235233\n",
            "Iteration 708, loss = 0.00240140\n",
            "Iteration 709, loss = 0.00230340\n",
            "Iteration 710, loss = 0.00232684\n",
            "Iteration 711, loss = 0.00231134\n",
            "Iteration 712, loss = 0.00245298\n",
            "Iteration 713, loss = 0.00233389\n",
            "Iteration 714, loss = 0.00243188\n",
            "Iteration 715, loss = 0.00231925\n",
            "Iteration 716, loss = 0.00223557\n",
            "Iteration 717, loss = 0.00226102\n",
            "Iteration 718, loss = 0.00227198\n",
            "Iteration 719, loss = 0.00225509\n",
            "Iteration 720, loss = 0.00229169\n",
            "Iteration 721, loss = 0.00226516\n",
            "Iteration 722, loss = 0.00228307\n",
            "Iteration 723, loss = 0.00226228\n",
            "Iteration 724, loss = 0.00222350\n",
            "Iteration 725, loss = 0.00224374\n",
            "Iteration 726, loss = 0.00217685\n",
            "Iteration 727, loss = 0.00218024\n",
            "Iteration 728, loss = 0.00218662\n",
            "Iteration 729, loss = 0.00219327\n",
            "Iteration 730, loss = 0.00220802\n",
            "Iteration 731, loss = 0.00215765\n",
            "Iteration 732, loss = 0.00218492\n",
            "Iteration 733, loss = 0.00214993\n",
            "Iteration 734, loss = 0.00211162\n",
            "Iteration 735, loss = 0.00213464\n",
            "Iteration 736, loss = 0.00216666\n",
            "Iteration 737, loss = 0.00214748\n",
            "Iteration 738, loss = 0.00220193\n",
            "Iteration 739, loss = 0.00208255\n",
            "Iteration 740, loss = 0.00213849\n",
            "Iteration 741, loss = 0.00208883\n",
            "Iteration 742, loss = 0.00215393\n",
            "Iteration 743, loss = 0.00208800\n",
            "Iteration 744, loss = 0.00213161\n",
            "Iteration 745, loss = 0.00225734\n",
            "Iteration 746, loss = 0.00200955\n",
            "Iteration 747, loss = 0.00218059\n",
            "Iteration 748, loss = 0.00210170\n",
            "Iteration 749, loss = 0.00212063\n",
            "Iteration 750, loss = 0.00203968\n",
            "Iteration 751, loss = 0.00205221\n",
            "Iteration 752, loss = 0.00203191\n",
            "Iteration 753, loss = 0.00207917\n",
            "Iteration 754, loss = 0.00207988\n",
            "Iteration 755, loss = 0.00201560\n",
            "Iteration 756, loss = 0.00203043\n",
            "Iteration 757, loss = 0.00210920\n",
            "Training loss did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MLPClassifier(hidden_layer_sizes=(20, 20), max_iter=1500, tol=1e-05,\n",
              "              verbose=True)"
            ],
            "text/html": [
              "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MLPClassifier(hidden_layer_sizes=(20, 20), max_iter=1500, tol=1e-05,\n",
              "              verbose=True)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MLPClassifier</label><div class=\"sk-toggleable__content\"><pre>MLPClassifier(hidden_layer_sizes=(20, 20), max_iter=1500, tol=1e-05,\n",
              "              verbose=True)</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "# Definir a arquitetura da rede neural para classificação\n",
        "# A arquitetura consiste em 3 camadas ocultas com 20 neurônios cada\n",
        "# A função de ativação usada é ReLU (Rectified Linear Unit)\n",
        "# A camada de entrada tem 3 neurônios (correspondentes às features)\n",
        "# A camada de saída tem 1 neurônio (para a classificação binária)\n",
        "rede_neural_credit = MLPClassifier(max_iter=1500, verbose=True, tol=0.0000100,\n",
        "                                   solver='adam', activation='relu',\n",
        "                                   hidden_layer_sizes=(20, 20))\n",
        "\n",
        "# Treinar a rede neural com os dados de treinamento\n",
        "rede_neural_credit.fit(X_credit_treinamento, y_credit_treinamento)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bmn64K7vqzx1",
        "outputId": "c825623f-d020-43a1-dbfb-1630c2df3989"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Previsões da rede neural nos dados de teste:\n",
            "  [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1\n",
            " 0 0 0 0 0 0 0 0 1 1 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 1 0 0 1 0\n",
            " 0 1 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 1 0\n",
            " 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0\n",
            " 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0\n",
            " 0 0 1 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 1 0 0 0 0 0 0 0 0 0 0 1 0\n",
            " 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 1 1 0 0 0 0 0 0 0 0 0 0 1\n",
            " 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 1 0 0 0 0 0 0 0 1 0 0 0 0 0 1 1 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0\n",
            " 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0\n",
            " 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0\n",
            " 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0\n",
            " 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 1 1]\n"
          ]
        }
      ],
      "source": [
        "# Fazer previsões usando a rede neural treinada nos dados de teste\n",
        "previsoes = rede_neural_credit.predict(X_credit_teste)\n",
        "print(\"Previsões da rede neural nos dados de teste:\\n \", previsoes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AzFRj9riq9gc",
        "outputId": "e05738c1-4c28-49d4-f0f5-a9a1016caee4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rótulos reais dos dados de teste:\n",
            " [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1\n",
            " 0 0 0 0 0 0 0 0 1 1 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 1 0 0 1 0\n",
            " 0 1 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0\n",
            " 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0\n",
            " 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0\n",
            " 0 0 1 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 1 0 0 0 0 0 0 0 0 0 0 1 0\n",
            " 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 1 1 0 0 0 0 0 0 0 0 0 0 1\n",
            " 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 1 0 0 0 0 0 0 0 1 0 0 0 0 0 1 1 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0\n",
            " 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0\n",
            " 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0\n",
            " 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0\n",
            " 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 1 1]\n"
          ]
        }
      ],
      "source": [
        "print(\"Rótulos reais dos dados de teste:\\n\", y_credit_teste)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4vRZgbSerBNW",
        "outputId": "73828d7f-45c9-47fa-cc43-d15b2daf9667"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precisão das previsões: 0.998\n"
          ]
        }
      ],
      "source": [
        "# Calcular e imprimir a precisão das previsões\n",
        "precisao = accuracy_score(y_credit_teste, previsoes)\n",
        "print(\"Precisão das previsões:\", precisao)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 496
        },
        "id": "XwtALw-Mronz",
        "outputId": "47e47ef1-dcaf-4afd-dadb-e04e84251363"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precisão da matriz de confusão: 0.998\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x550 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Criar uma instância da matriz de confusão para o classificador de rede neural\n",
        "cm = ConfusionMatrix(rede_neural_credit)\n",
        "\n",
        "# Treinar a matriz de confusão com os dados de treinamento\n",
        "cm.fit(X_credit_treinamento, y_credit_treinamento)\n",
        "\n",
        "# Avaliar a precisão da matriz de confusão com os dados de teste\n",
        "precisao_cm = cm.score(X_credit_teste, y_credit_teste)\n",
        "print(\"Precisão da matriz de confusão:\", precisao_cm)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rWj1N8ANsYrI",
        "outputId": "4a668432-f327-4991-8fb3-f23a8fc4875e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00       436\n",
            "           1       0.98      1.00      0.99        64\n",
            "\n",
            "    accuracy                           1.00       500\n",
            "   macro avg       0.99      1.00      1.00       500\n",
            "weighted avg       1.00      1.00      1.00       500\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Imprimir o relatório de classificação para avaliar o desempenho do classificador\n",
        "print(\"Relatório de Classificação:\\n\", classification_report(y_credit_teste, previsoes))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **Precisão (Precision)**:\n",
        "  - Para a classe \"0\" (baixo risco de crédito), o modelo previu corretamente todas as instâncias. Isso significa que todas as vezes em que o modelo disse que um cliente tinha baixo risco de crédito, ele estava correto.\n",
        "  - Para a classe \"1\" (alto risco de crédito), o modelo previu corretamente a grande maioria das instâncias. No entanto, houve algumas instâncias em que o modelo previu alto risco quando, na verdade, não era o caso.\n",
        "\n",
        "- **Revocação (Recall)**:\n",
        "  - Tanto para a classe \"0\" quanto para a classe \"1\", o modelo identificou corretamente todas as instâncias. Isso significa que ele não deixou passar nenhum caso de baixo ou alto risco de crédito.\n",
        "\n",
        "- **F1-Score**:\n",
        "  - Para a classe \"0\", o F1-score é de 100%, o que indica um excelente equilíbrio entre precisão e recall. Isso significa que o modelo está acertando todas as previsões de baixo risco sem deixar nenhum caso de fora.\n",
        "  - Para a classe \"1\", o F1-score é de 99%, o que é muito bom, mas sugere que há um pequeno número de falsos positivos, onde o modelo está prevendo alto risco de crédito quando não é o caso.\n",
        "\n",
        "- **Acurácia (Accuracy)**:\n",
        "  - A acurácia do modelo é de 100%, o que significa que ele está acertando todas as previsões, tanto para baixo quanto para alto risco de crédito. Isso é uma ótima notícia, indicando que o modelo está fazendo um trabalho excepcionalmente bom em classificar os clientes com base no risco de crédito.\n",
        "\n"
      ],
      "metadata": {
        "id": "SuN1z74U1v2l"
      }
    }
  ]
}